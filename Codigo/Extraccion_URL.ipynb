{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Paginas Web de la facultad de matemáticas obtenidas en el día 23 de junio de 2024"
      ],
      "metadata": {
        "id": "uAduAHt4Dmdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalar libreria necesaria"
      ],
      "metadata": {
        "id": "xHbBv1Rg5Bx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install scrapy"
      ],
      "metadata": {
        "id": "kAgj2X1pc6Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear nuevo proyecto"
      ],
      "metadata": {
        "id": "VtxB0VJJ5Gq-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-s9_QOiLvjL"
      },
      "outputs": [],
      "source": [
        "!scrapy startproject webcrawler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyIgihsFMQKt"
      },
      "source": [
        "## Obtener direcciones web a partir de URL inicial"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear archivo con el código del algoritmo apra encontrar las direcciones web"
      ],
      "metadata": {
        "id": "6ONHW_UQ5XPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8mZiJtOMQK0"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/webcrawler/webcrawler/spiders/url_spider.py\n",
        "import scrapy\n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "from scrapy.spiders import CrawlSpider, Rule\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "class UrlSpider(CrawlSpider):\n",
        "    name = 'url_spider'\n",
        "    allowed_domains = ['matematicas.ucm.es']\n",
        "    start_urls = ['https://matematicas.ucm.es/']  # Página inicial\n",
        "\n",
        "    custom_settings = {\n",
        "        'DEPTH_LIMIT': 4  # Limita la profundidad del rastreo\n",
        "    }\n",
        "\n",
        "    rules = (\n",
        "        Rule(LinkExtractor(allow=()), callback='parse_item', follow=True),\n",
        "    )\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(UrlSpider, self).__init__(*args, **kwargs)\n",
        "        self.urls_seen = set()\n",
        "\n",
        "    def parse_item(self, response):\n",
        "        urls = response.css('a::attr(href)').extract()\n",
        "        absolute_urls = [urljoin(response.url, url) for url in urls]\n",
        "\n",
        "        with open('extracted_urls.txt', 'a') as f:\n",
        "            for url in absolute_urls:\n",
        "                if url not in self.urls_seen:\n",
        "                    self.urls_seen.add(url)\n",
        "                    f.write(f\"{url}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutar algoritmo de búsqueda"
      ],
      "metadata": {
        "id": "09Wax7k36DAq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD-mn2hfMQK0"
      },
      "outputs": [],
      "source": [
        "%cd /content/webcrawler\n",
        "!scrapy crawl url_spider\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizar las direcciones obtenidas"
      ],
      "metadata": {
        "id": "sTZSOd4K6Hgm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hYVUOKXMQK0"
      },
      "outputs": [],
      "source": [
        "# Leer las URLs extraídas\n",
        "with open('extracted_urls.txt', 'r') as f:\n",
        "    urls = f.read().splitlines()\n",
        "\n",
        "print(len(urls))\n",
        "print(urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MjzbhThLvjM"
      },
      "source": [
        "## Obtener matriz de adyacencia de webs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear archivo con el código del algoritmo apra encontrar las direcciones web y crear la matriz de adaycencia"
      ],
      "metadata": {
        "id": "mw4Ij7lU6plw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClWUDRukLvjM"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/webcrawler/webcrawler/spiders/link_spider.py\n",
        "import scrapy\n",
        "from scrapy.spiders import CrawlSpider, Rule\n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Leer las URLs extraídas\n",
        "with open('extracted_urls.txt', 'r') as f:\n",
        "    initial_urls = f.read().splitlines()\n",
        "\n",
        "\n",
        "class LinkSpider(CrawlSpider):\n",
        "    name = 'link_spider'\n",
        "    allowed_domains = ['matematicas.ucm.es']\n",
        "\n",
        "    # Lista de URLs\n",
        "    start_urls = initial_urls\n",
        "\n",
        "    custom_settings = {\n",
        "        #'DEPTH_LIMIT': 1,  # Limita la profundidad del rastreo\n",
        "        #'CLOSESPIDER_PAGECOUNT': 100  # Limita el número de páginas rastreadas\n",
        "    }\n",
        "\n",
        "    rules = (\n",
        "        Rule(LinkExtractor(allow=()), callback='parse_item', follow=False),\n",
        "    )\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(LinkSpider, self).__init__(*args, **kwargs)\n",
        "        self.url_to_index = {}\n",
        "        self.index_to_url = []\n",
        "        self.adjacency_list = []\n",
        "\n",
        "        # Crear índice para las URLs iniciales\n",
        "        for url in self.start_urls:\n",
        "            if url not in self.url_to_index:\n",
        "                self.url_to_index[url] = len(self.index_to_url)\n",
        "                self.index_to_url.append(url)\n",
        "                self.adjacency_list.append([])\n",
        "\n",
        "    def parse_item(self, response):\n",
        "        page_url = response.url\n",
        "        if page_url in self.url_to_index:\n",
        "            page_index = self.url_to_index[page_url]\n",
        "            links = LinkExtractor(allow=()).extract_links(response)\n",
        "            for link in links:\n",
        "                target_url = link.url\n",
        "                if target_url in self.url_to_index:\n",
        "                    target_index = self.url_to_index[target_url]\n",
        "                    self.adjacency_list[page_index].append(target_index)\n",
        "\n",
        "    def closed(self, reason):\n",
        "        size = len(self.index_to_url)\n",
        "        adjacency_matrix = np.zeros((size, size))\n",
        "        for i, targets in enumerate(self.adjacency_list):\n",
        "            for target in targets:\n",
        "                adjacency_matrix[i, target] = 1\n",
        "\n",
        "        df = pd.DataFrame(adjacency_matrix, index=self.index_to_url, columns=self.index_to_url)\n",
        "        df.to_csv('adjacency_matrix.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutar algoritmo de búsqueda"
      ],
      "metadata": {
        "id": "NEwvUc3g6pl3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0K_COgGLvjM"
      },
      "outputs": [],
      "source": [
        "%cd /content/webcrawler\n",
        "!scrapy crawl link_spider"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizar matriz de adyacencia obtenida"
      ],
      "metadata": {
        "id": "gr-0a-xB7Q_L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saCrOlcrLvjN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Leer la matriz de adyacencia\n",
        "df = pd.read_csv('/content/webcrawler/adjacency_matrix.csv', index_col=0)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.index)"
      ],
      "metadata": {
        "id": "x8Ta0jODKToj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}